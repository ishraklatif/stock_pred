<!-- HERO SECTION -->
<h1 align="center">ğŸ“ˆ StockPred</h1>
<h3 align="center">Multiseries PatchTST-based ASX Market Forecasting</h3>

<p align="center">
  <img src="https://img.shields.io/badge/Model-PatchTST-blue" />
  <img src="https://img.shields.io/badge/Framework-PyTorch%20Lightning-orange" />
  <img src="https://img.shields.io/badge/Forecasting-Multiseries-success" />
  <img src="https://img.shields.io/badge/Status-Active-brightgreen" />
</p>

---

# ğŸŒŸ Overview

**StockPred** is a research-grade and production-oriented multiseries forecasting system built on the state-of-the-art **PatchTST (Time-Series Patch Transformer)** architecture.

It predicts **next-day stock prices for ASX companies** using a rich blend of:

- Company OHLCV  
- Global macro indicators  
- Commodities & FX  
- Calendar & holiday effects  
- 140+ technical indicators  
- News sentiment  
- Sector embeddings  
- Cross-series temporal dependencies  

The project delivers a fully modular, extensible forecasting pipeline for **academic research**, **industry forecasting**, and **portfolio-quality AI engineering**.

---

# ğŸ¯ Motivation

Financial forecasting demands models capable of handling:

- Multiseries correlations  
- Volatile and non-stationary regimes  
- High-dimensional feature spaces (1200+ engineered signals)  
- Multi-horizon forecasting  
- Irregular and long-range temporal structure  

Classical models fail at scale.  
Transformers solve this â€” but standard attention scales poorly with very high feature counts.

**PatchTST** offers the advantages of Transformers without the feature explosion:

- Tokenization over *time patches* (not over features)  
- Shared encoders across features â†’ prevents overfitting  
- Excellent stability with 1000+ time-varying features  
- SOTA performance across forecasting benchmarks  

StockPred leverages PatchTST to achieve stable and scalable forecasting performance across 20â€“100 tickers.

---

# ğŸ§  PatchTST Architecture

PatchTST transforms time-series segments into patch embeddings and processes them with a transformer encoder.

### ğŸ“ High-Level Architecture

```text
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Raw Time Series Segments         â”‚
                    â”‚ (per feature, per time window)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚           Patch Extraction          â”‚
                    â”‚   (e.g., length=32, stride=16)      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Patch Embedding (Shared Encoder) â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     Transformer Encoder Layers     â”‚
                    â”‚   (Self-Attention + FFN blocks)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     Multi-horizon Predictions      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why PatchTST for StockPred?

- Scales easily to **1200+ numerical features**  
- Robust for **multiseries forecasting**  
- Outperforms temporal CNNs, TFT, and Informer in many benchmarks  
- Stable even with small prediction windows (e.g., 1â€“5 days)  
- Avoids feature-selection overhead of TFT  

---

# ğŸ“ Project Structure

```text
project/
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ train_patchtst.yaml
â”‚   â”œâ”€â”€ config-search-patchtst.yaml
â”‚   â””â”€â”€ data.yaml
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_companies/
â”‚   â”œâ”€â”€ raw_macro/
â”‚   â”œâ”€â”€ raw_macro_market/
â”‚   â”œâ”€â”€ processed_companies/
â”‚   â”œâ”€â”€ processed_macro/
â”‚   â”œâ”€â”€ processed_macro_market/
â”‚   â””â”€â”€ tft_ready_multiseries/
â”‚       â”œâ”€â”€ train.parquet
â”‚       â”œâ”€â”€ val.parquet
â”‚       â””â”€â”€ test.parquet
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ clean/
â”‚   â”‚   â”œâ”€â”€ company_clean.py
â”‚   â”‚   â”œâ”€â”€ macro_clean.py
â”‚   â”‚   â””â”€â”€ market_clean.py
â”‚   â”‚
â”‚   â”œâ”€â”€ compute/
â”‚   â”‚   â”œâ”€â”€ compute_calendar_features.py
â”‚   â”‚   â”œâ”€â”€ compute_indicators.py
â”‚   â”‚   â””â”€â”€ compute_news_sentiment.py
â”‚   â”‚
â”‚   â”œâ”€â”€ fetch/
â”‚   â”‚   â”œâ”€â”€ fetch_company.py
â”‚   â”‚   â”œâ”€â”€ fetch_macro_main.py
â”‚   â”‚   â”œâ”€â”€ fetch_macro_market.py
â”‚   â”‚   â””â”€â”€ fetch_macro_news.py
â”‚   â”‚
â”‚   â”œâ”€â”€ merge/
â”‚   â”‚   â””â”€â”€ merge_all_data.py
â”‚   â”‚
â”‚   â”œâ”€â”€ prepare_data_tft.py
â”‚   â”œâ”€â”€ inspect_data_by_pipeline.py
â”‚   â”œâ”€â”€ test_tickers.py
â”‚   â”‚
â”‚   â”œâ”€â”€ train_patchtst.py
â”‚   â”œâ”€â”€ hparam_search_patchtst.py
â”‚   â””â”€â”€ evaluate_patchtst.py
â”‚
â””â”€â”€ checkpoints_patchtst/
```

---

# ğŸ”§ Feature Engineering Pipeline

StockPred constructs a **rich, high-dimensional multiseries dataset** from:

- Company OHLCV  
- Macro indices (S&P500, FTSE, Nikkei, etc.)  
- Market-level signals (DXY, Gold, Brent, AUD/USD, VIXâ€¦)  
- 140+ technical indicators  
- Calendar seasonalities  
- Business days, holidays, month-end/quarter-end  
- News sentiment via FinBERT  
- Sector embeddings  

---

# ğŸš€ Usage Guide

## 1. Install Dependencies

```bash
pip install -r requirements.txt
```

## 2. Prepare Dataset

```bash
python scripts/prepare_data_tft.py
```

## 3. Train PatchTST Model

```bash
python scripts/train_patchtst.py
```

## 4. Run Hyperparameter Search

```bash
python scripts/hparam_search_patchtst.py
```

## 5. Evaluate the Model

```bash
python scripts/evaluate_patchtst.py
```

---

# ğŸ“š References

- Nie, Y. et al. **"Time Series Patching Transformer"**, NeurIPS 2023  
- Zerveas, G. et al. **"A Transformer-based Framework for Multivariate Time Series Representation Learning"**, ICLR 2021  
- Lim, B., Arik, S. Ã–. **"Temporal Fusion Transformers"**, NeurIPS 2019  

---

# ğŸ Conclusion

StockPred now leverages **PatchTST**, enabling scalable, stable multiseries forecasting across high-dimensional datasets.  
This modernized pipeline is suitable for academic research and industry-level forecasting deployments.
